---
title: "R Study Guide for Cronkite School of Journalism"
author: "Sarah Cohen, Knight Chair in Journalism"
url: "https://cronkitedata.github.io/rstudyguide"
github-repo: "cronkitedata/rstudyguide"
favicon: "favicon.io" 
description: "A companion book for the Cronkite School's data journalism class for graduate students. A work in progress."
---
---
title: "R Study Guide for Cronkite School of Journalism"
author: "Sarah Cohen, Knight Chair in Journalism"
url: "https://cronkitedata.github.io/rstudyguide"
github-repo: "cronkitedata/rstudyguide"
favicon: "favicon.io" 
description: "A companion book for the Cronkite School's data journalism class for graduate students. A work in progress."
---

# Preface {-}

This online book is serving as a study guide for students in my data journalism classes at the Cronkite School of Journalism. Eventually, this will put together the resources for the whole class, but for now it just includes the section on learning R and R Markdown. 

I'm grateful for all of the experts and teachers out there who have made their training materials open to the world, and helped build a wealth of materials for students of all types. I've particularly leaned on Matt Waite, of the University of Nebraska and Jesse Lecy of Arizona State University, who generously share their course materials.

<!--chapter:end:index.Rmd-->


# (PART) Introduction to R for Journalists {-}
# An R and R Studio tour {#rstudio-tour}

Placeholder


## Key takeaways
## Why program?
#### Why R? {-}
## Setting up R and R Studio 
#### RStudio Cloud {-}
#### Install yourself {-}
## An R Studio tour
### The Console 
### Work in projects 
## Unlocking packages
#### Packages for this class {-}
#### A quick note on the tidyverse {-}
## Variables
#### Using variables in new variables {-}
#### Working with lists {-}
## Resources and exercises
#### Take the swirl() tutorial {-}
### Googling for help
#### Other resources {-}

<!--chapter:end:011-r-install.Rmd-->


# R Markdown 

Placeholder


## Key takeaways
## Getting started with R Markdown
## The anatomy of a document
#### Front matter / YAML at the top {-}
## Knitting your document
## Code chunks
#### R Markdown starts from scratch {-}
## Resources and exercises
### Resources {-}
### Exercises {-}
## Bonus: Styling your R Markdown documents

<!--chapter:end:021-r-markdown.Rmd-->


# Data frames and the tidyverse

Placeholder


## Key takeaways
## Getting started
## The data frame
### Loading and looking at data
### Examine your data
## Introducing the pipe
## Take a break
## Other resources
### On your own

<!--chapter:end:022-data-frames.Rmd-->


# (PART) The Verbs of the Tidyverse {-}
# A quick tour of verbs

Placeholder


## Key takeaways 
### Data for this walkthrough {-}
### Follow along
### The `%>%` operator: ... and then... {-}
##  Looking at your data in pieces and in order
### `select` to pick out columns
### `arrange` to sort 
### `filter` to pick out rows
## `group_by` and `summarise` to count and sum
## `mutate` to calculate something new
## `join` to merge tables together by column; `union` to add rows

<!--chapter:end:030-intro-verbs.Rmd-->


# Verbs Part 1: Picking and choosing 

Placeholder


## Key takeaways 
## Select - choosing columns
### Optional: Other ways to select {-}
## Filter - choosing rows
### One filter
### A compound filter
### Even more compound filters
#### OR conditions {-}
#### %in% conditions {-}
## Arrange - change the order
## Traps
#### Case-sensitivity {-}
#### Equals versus double-equals {-}
#### The "and then" operator (%>%) {-}
#### The dreaded NA {-}
## Resources
### Exercises
### Tutorials

<!--chapter:end:031-select-filter.Rmd-->


# Verbs 2: group_by and summarise

Placeholder


## Key takeaways
## group_by / summarise^[The tidyverse uses the British spelling of a lot of words. You can use the American spelling, but you'll usually see it this way.] basics
### Other summary functions
### Grouping by more than one variable
### Rolling up to the next level
#### A note on count() and tally() {-}
## Restructuring for a report with `pivot_wider()`
### A note on restructuring your data frames
## Resources & exercises
### Exercises

<!--chapter:end:033-groupby.Rmd-->


# Verbs 3: mutate

Placeholder


## Key takeaways
## Read in some data
## Simple arithmetic
## Creating categories 
## Bonus: Fun with date and time

<!--chapter:end:035-mutate.Rmd-->


# (APPENDIX) Appendix {-}
# A gentle intro to programming  

Placeholder


## Key takeaways
## Building blocks
### Variables and objects {-}
### Operators & assignment {-}
### Functions {-}
### Loops {-}
## Make an omelet with a function
### Open a restaurant with loops {-}
## Applications
## More resources

<!--chapter:end:A02-programming.Rmd-->

<!-- Add link to the drag queen coding class. See if there are other good first-day programming aides I should add.-->
# Great R Resources

## Key takeaways

* If this set of resources doesn't fit your learning style, there are lots of others that might
* In our class, we are working almost entirely in the "tidyverse", and in R Markdown documents. Some of the resources you'll find elsewhere might not.

## Coursework in R 

Other professors and universities are also trying to teach R in journalism and other classes. Here are a few that have particularly rich materials for their students: 

* Jesse Lecy's "Introduction to data science for the social sector,"  an ASU course that teaches R in the context of non-profit management. His site, <https://ds4ps.org/cpp-526-fall-2019/> has a [custom-made textbook](https://ds4ps.org/cpp-526-fall-2019/textbook/) that covers much of the same material as us. 

* Matt Waite at the University of Nebraska has been teach R in the context of a sports journalism class. [His textbook](http://mattwaite.github.io/sports/index.html) also has great resources and approaches to learning. One minor difference that might trip you up: We're using R Markdown, and he's using R Notebooks. They're very similar, but the "knit" option isn't there when you use Notebooks -- only a "Preview". You'll know what that means when we get moving.

## Instructional screencasts and videos

* Andrew Ba Tran's [R for Journalists](http://learn.r-journalism.com/en/) course, originall created for a MOOC offered by the  Knight Center for the Americas. Previous Cronkite students say that it goes a little fast for someone with no experience in other lanugages, but it has most of what you'd need.

* Ben Stenhaug, a Data Science for Social Good fellow at Stanford, has put together some screencasts and exercises called [teachR](https://teachingr.com/) on starting out in R and the tidyverse.

* "Tidy Tuesday" screencasts from David Robinson, [an example]((https://www.youtube.com/channel/UCeiiqmVK07qhY-wvg3IZiZQ)) using fivethirtyeight.com data on education and salaries. These are very long -- up to an hour -- but they walk through what it's really like to take a dataset from scratch and try to find something interesting. I disagree with one thing he does though -- I believe you should read the documentation FIRST, not after you've done a lot of guessing about what columns mean.

* Sharon Machliss' "[Do More with R](https://www.youtube.com/playlist?list=PL7D2RMSmRO9JOvPC1gbA8Mc3azvSfm8Vv)" YouTube series. These are fairly advanced single-topic, short videos.



<!--chapter:end:A01-resources.Rmd-->

# Murder Accountability Project exercises


<style>
   table {
      font-size:.8em;
   }
</style>


The Murder Accountability Project was profiled in the [New Yorker in November, 2017](https://www.newyorker.com/magazine/2017/11/27/the-serial-killer-detector). Andrew B Tran brilliantly decided to use its data as a vehicle for learning R in his [R for Journalists' online class](https://learn.r-journalism.com/en/).  This set of tutorials adapts his exercises for people using this textbook. You should consider taking his entire free full course if you want to get more detail. 


[Download the data](https://github.com/cronkitedata/rstudyguide/blob/master/data/murder_data.Rda) into a new or existing R project, then load it using the `load(file="murder_data.Rda")` command in a code chunk. 


```{r include=FALSE}

library(tidyverse)
#optional, for fancy tables
library (DT)
# optional, for crosstabs
library (janitor)
# more fancy tables
library (knitr)
load("data/murder_data.Rda")


```


## The data

This R dataset was created by subsetting only mountain states and Oregon and Washington from the national dataset, and converting the codes used in the original SPSS data into codes and their labels in separate variables. The data runs from 1976 through 2018. Here are the fields included: 


```{r echo=FALSE}
glimpse(murder_data)

```

Any variable that begins with `off` refers to the offender; any variable that begins with `vic` refers to the victim. Most variables are pretty self-explanatory, but here are a few details: 

* `fip` and `state_fip` are standard codes used across many databases to identify geographic areas -- in this case, counties and states.
* `msa_code` and `msa_name` refer to Metropolitan Statistical Areas, which combine nearby counties regardless of state into areas that are often considered on large metropolitan area, such as Washington DC and the Virginia and Maryland suburbs.
* `ori`, `agency` and `agency_type` refer to the law enforcement agency -- police or sheriff's office -- that investigated the murder. An ORI is a standard code for each agency from the FBI. 
* `relationship_code` and `relationship` can be confusing. They refer to the relationship of the victim to the offender, not the other way around. For example, "Wife" means that the victim was the killer's wife, not that the killer was the victim's wife. This is unclear in the FBI documentation. 

More details are available in [this detailed record layout and data dictionary](https://www.dropbox.com/s/lo6tgo8nnbpqeru/MAPdefinitionsSHR.pdf?dl=1). 



## Exercises by chapter

### Select and filter exercises

Here are some suggested exercises to practice what you learned in Chapter 3.1, Select and Filter: 


#### Older wives as victims in Arizona {-}

1. Create a new data frame called `arizona_murders` based on just the murders that were reported in that state.  

Try doing these one step at a time by adding to a query : 

2. Pick out just following variables to work with:  
    * year
    * name of the county and the police department, 
    * whether it was solved, 
    * demographics (eg, age, race, sex, ethnicity) of the victim and the offender, 
    * information on the weapon and the relationship
    
3. List all of the murders in which the killer was the husband of the victim. This can be done using either the relationship or the relationship_code.

4. Add a condition that the wives were at least 60 years old. 

5. Sort the answer by oldest to youngest


#### Gun-related killings {-}

Finding out what codes are in the data could be done with a `group_by`/`summarise` query, but there is another verb that can show you every **unique** value in a dataset. Use this code to show you every type of weapon used in the dataset: 

        murder_data %>%
           distinct (weapon_code, weapon) %>%
           arrange (weapon_code)


The verb `distinct` is used instead of `select` to just show a list of values that are never repeated. When you run that code, you can see why it's sometimes useful to keep codes as well as words in a dataset -- codes "11" through "15" refer to some kind of gun. 

**Q: Find all gun-related murders of young black or Hispanic men since 2015.
You can define "young" however you want, but in my example I'll use victims between 15 and 29. 

In this example, you'll have to combine OR conditions, with others. Remember you can use BETWEEN for ranges of values or %in% for a list of values. 


#### Advanced exercises using other conditions {-}

If you're feeling adventurous, try figuring out how you might find: 

* Any domestic-violence related incidents. Hint: This would be an `%in%` condition once you look at your options using either "distinct" or a group-by query. 

* Try using `str_detect` when you want to use wild cards instead of exact matches. These take regular expressions as arguments. So to find any gun in this dataset, you'd use 

      str_detect(weapon_code, "^1")

(For more details on regular expressions, try the [Regex101 tutorial](https://cronkitedata.github.io/cronkite-docs/special/regex-beginning.html) on our class website. Using regular expressions is often a way to make queries shorter and less fussy, but they are not as clear to a reader -- they often take some puzzling through.)


### Group by and summarise

The group by exercises are just like a pivot table. In fact, to turn it on its head, you use the command "pivot_wider"


#### "The most" {-}

* Which county in this small dataset has the most murders? Which one has the most police killings? (Look in the circumstance column for this.)

* Create a table showing the number of murders by year and state (with states across the top, and years down the side). This is a group_by / summarise / arrange / pivot_wider exercise

* Try calculating the percent of murders by relationship. For this to work, you can only keep one group_by column (relationship)

#### Putting it all together {-}

What percent of each state's domestic violence victims are of Hispanic origin? 


####  Bonus: Mutate with group_by {-}

Chapter 3.4, Mutate, has an example of creating a new category out of an old one. Try to puzzle through how to find the county that has the highest rate of gun murders vs. other weapons. This involves three steps: creating a new category of Yes/No out of the weapon; grouping and counting; then rolling up to the next level. 

## Answers to exercises

### Select and filter

#### Arizona wives {-}

1. Create a new dataset with just arizona: 

```{r}
arizona_murders <- 
  murder_data %>%
  filter ( state_abbr == "AZ")


```


You should have 15,443 rows in this dataset.

2. The final set of queries might look like this. (You might have noticed you that you had one victim age 999 when you sorted. That means "unknown" in this dataset, so you'll want to filter that out as well.)

```{r}

arizona_murders  %>%
  select ( year, cnty_name, agency, solved, 
           starts_with("vic"), starts_with("off"), 
           contains ("relationship")) %>%
  filter (relationship == "Wife"  & 
          vic_age >= 60 & 
          vic_age < 999 )  %>%
  arrange ( desc(vic_age)) 

  

```

You could also use 
      
      relationship == "Wife"  & 
          between (vic_age, 60, 998)  
          

You might also notice that there is a Male "wife" as a victim, reflecting how poorly many police agencies fill out these forms. 


#### Gun-related killings {-}

This is actually much more difficult than it sounds. Try to build it one piece at a time.  Here is how I might build the conditions: 
Guns:

        ... weapon_code %in% c("11", "12", "13", "14", "15")

(This is a text variable even though it looks like numbers - that means you need the quotes. Don't forget to use the "c" for "combine into a list" before the list of values)^[ If you wanted to go further with filtering, you might look at the `regular expressions` available for more sophisticated filtering using the `str_detect` function. In this case,  `str_detect (weapon_code, "^1")` searches for anything in the field that begins with a "1".]

Since 2015:

        weapon_code %in% c("11", "12", "13", "14", "15") &
        year >= 2015


Young men: 

        weapon_code %in% c("11", "12", "13", "14", "15") &
        year >= 2015 & 
        between (vic_age, 15, 29)

Black and Hispanic victims:

        weapon_code %in% c("11", "12", "13", "14", "15") &
        year >= 2015 & 
        between (vic_age, 15, 29)  &
        (vic_race_code  == "B" | vic_ethnic_code = "H")
        

That last one is the trickiest -- If you want to find both African-American AND Hispanic victims, you need to look for a race code of "B" OR an ethnicity code of "H". Those have to be in a parenthese in order not to be confused with the other conditions. (I'm going to select just some of the columns and order it by the year and month of the murder, showing only the most recent). (NOTE: Eliminate the last line of this code if you haven't installed and activated the "DT" package, which makes searchable, sortable tables.)  


```{r}

murder_data %>%
  filter ( weapon_code %in% c("11", "12", "13", "14", "15") &
           year >= 2015 & 
           between (vic_age, 15, 29)  &
           (vic_race_code  == "B" | vic_ethnic_code == "H")
        ) %>%
  select (year, month, cnty_name, state_abbr, solved, vic_age, vic_race, vic_ethnic,  weapon, relationship, circumstance ) %>%
  arrange (year, month) %>%
  tail (50) %>%
  datatable( options = list(scrollX = '500px'))


```

If this complex filter seems too complex, you can always chain one filter after another to check your data as you go. For example, you could do this: 

         murder_data %>%
            filter ( weapon_code %in% c("11", "12", "13", "14", "15") ) %>%
            filter ( vic_race_code == "B" | vic_ethnic_code == "H") ....
            
            
            
### Group by 


#### "The most" {-}


1. County with the most killings
<<<<<<< HEAD

```{r}

murder_data %>%
  group_by (state_abbr, cnty_name) %>%
  summarise ( cases = n() ) %>%
  arrange (desc (cases))

```

Why is this answer not surprising? (Hint: Clark County, Nevada, has about half the population of Maricopa.) We'll get to ways to normalize this in later chapters.

To just get the police shootings, filter the above query for `circumstance_code == "81"` before the `group_by`

2. Murders by state and year

(I'm putting the most recent year at the top)

```{r}


murder_data %>% 
  group_by (year, state_abbr) %>%
  summarise (cases = n() ) %>%
  #bonus: Calculate the total number of cases by year:
  mutate ( total_cases = sum(cases) )  %>%
  pivot_wider (names_from = state_abbr, values_from = cases) %>%
  arrange ( desc (year)) %>%
  #Extra! fancy table that requires the DT package.
  datatable( options = list(scrollX= "400px", 
                            pageLength = "5", 
                            lengthMenu = c(5, 10, 50)
                            )
             )
  
```
Does this mean that these states have suddenly become more dangerous? What about population growth? 

**Alternative method**

The `janitor` package (which you may need to install) has a way to create cross-tabulations like this more simply. The function is `tabyl` (to distinguish from other table operations in R, which you probably want to avoid).  Here's an example: 

```{r}

murder_data %>%
  tabyl ( year , state_abbr) %>%
  arrange ( desc (year) ) %>%
  head () %>%
  #to make a nicer looking table, with the knitr() package
  kable()


```




3. Percent of murders by relationship:

```{r}

murder_data %>%
  group_by (relationship) %>%
  summarise (num_of_cases = n() ) %>%
  mutate (total_cases = sum(num_of_cases) ,
          # This rounds to 1 digit.
          pct_cases = round (num_of_cases / total_cases * 100 , 1)
          ) %>%
  # you could un-select the total cases since they'll always be the same , but for illustratio purposes I'm keeping it.
  arrange ( desc (num_of_cases)) 
```

**Alternative method with `janitor::tabyl`**
=======
>>>>>>> fd6bece1b050ebecb7554e7277de5a95955c6313

```{r}

murder_data %>%
<<<<<<< HEAD
  tabyl (relationship) %>%
  adorn_pct_formatting (digits=1)   %>%
  #this last part turns it into a normal data frame
  arrange ( desc(n)) %>%
  as_tibble()

=======
  group_by (state_abbr, cnty_name) %>%
  summarise ( cases = n() ) %>%
  arrange (desc (cases))

```

Why is this answer not surprising? (Hint: Clark County, Nevada, has about half the population of Maricopa.) We'll get to ways to normalize this in later chapters.

To just get the police shootings, filter the above query for `circumstance_code == "81"` before the `group_by`

2. Murders by state and year

(I'm putting the most recent year at the top)

```{r}


murder_data %>% 
  group_by (year, state_abbr) %>%
  summarise (cases = n() ) %>%
  #bonus: Calculate the total number of cases by year:
  mutate ( total_cases = sum(cases) )  %>%
  pivot_wider (names_from = state_abbr, values_from = cases) %>%
  arrange ( desc (year)) %>%
  #Extra! fancy table that requires the DT package.
  datatable( options = list(scrollX= "400px", 
                            pageLength = "5", 
                            lengthMenu = c(5, 10, 50)
                            )
             )
  
```
Does this mean that these states have suddenly become more dangerous? What about population growth? 

NOTE: Later on, we'll get to other packages, which includes `janitor` --  a useful way to clean data. That package has a much easier way to produce tables while formatting them well. If you've already installed and loaded the package, here's another way to do the same thing: 


```{r}

murder_data %>%
  tabyl ( year , state_abbr) %>%
  arrange ( desc (year) ) %>%
  head () %>%
  kable()


```




3. Percent of murders by relationship:

```{r}

murder_data %>%
  group_by (relationship) %>%
  summarise (num_of_cases = n() ) %>%
  mutate (total_cases = sum(num_of_cases) ,
          # This rounds to 1 digit.
          pct_cases = round (num_of_cases / total_cases * 100 , 1)
          ) %>%
  # you could un-select the total cases since they'll always be the same , but for illustratio purposes I'm keeping it.
  arrange ( desc (num_of_cases)) 
```

Again, using the `tabyl` function from the `janitor` package:

```{r df_print="paged"}

murder_data %>%
  tabyl (relationship) %>%
  adorn_pct_formatting (digits=1)  %>%
  kable()
  
>>>>>>> fd6bece1b050ebecb7554e7277de5a95955c6313
```




#### Putting it all together {-}
  
  First, isolate the domestic violence cases. Let's see what our choices are: 
  
```{r}

murder_data %>%
  group_by (relationship_code, relationship) %>%
  summarise (n())

```

Here's one way to get the answer (eliminating cases in which we don't know the victim's ethnicity)

  
```{r}

murder_data %>%
  filter ( relationship_code %in% c("BF", "BR", "CH", "CW", "DA", "FA", 
                                    "GF", "HO", "HU", "IL", "MO", "OF", "SD", "SF", 
                                    "SI", "SM", "SO", "WI", "XH", "XW")   & 
          vic_ethnic_code %in% c("H", "N")
        ) %>%
   group_by ( state_abbr, vic_ethnic ) %>%
   summarise (cases = n() ) %>%
   #calculate total by state and percent
   mutate ( total_cases = sum(cases), 
            pct_cases = cases / total_cases * 100 ) %>%
   #get rid of case counts
   select ( -cases ) %>%
   #sort by state
   arrange ( state_abbr ) %>%
   # put ethnicity in columns
   pivot_wider ( values_from = pct_cases, names_from = vic_ethnic)  
  


```
  
<<<<<<< HEAD
**Alternative  Using the `janitor::tabyl` function**
=======
  Using the `tabyl` package it would be easier: 
>>>>>>> fd6bece1b050ebecb7554e7277de5a95955c6313
  
```{r}

murder_data %>%
  filter ( relationship_code %in% c("BF", "BR", "CH", "CW", "DA", "FA", 
                                    "GF", "HO", "HU", "IL", "MO", "OF", "SD", "SF", 
                                    "SI", "SM", "SO", "WI", "XH", "XW")   & 
          vic_ethnic_code %in% c("H", "N")
        ) %>% 
  tabyl ( state_abbr, vic_ethnic) %>%
<<<<<<< HEAD
  # include column and row totals
  adorn_totals ( c("row", "col")) %>%
  #calculate % of row total (within state)
  adorn_percentages ( "row") %>%
  # remove decimal places
  adorn_pct_formatting (digits=1) %>% 
  # include # of cases
  adorn_ns %>%
  # turn it into a regular data frame
  as_tibble()
=======
  adorn_totals ( c("row", "col")) %>%
  adorn_percentages ( "row") %>%
  adorn_pct_formatting (digits=1) %>% 
  adorn_ns %>%
  kable()
>>>>>>> fd6bece1b050ebecb7554e7277de5a95955c6313


```
  
  

<!--chapter:end:A04-murders.Rmd-->

